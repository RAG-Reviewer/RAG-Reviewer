{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99063ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from transformers.utils import logging\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1982177",
   "metadata": {},
   "source": [
    "This section defines the required setup for fune-tuning. Users must manually set the `model_name` and `rag_strategy` variables in the code (modify them in the below section).\n",
    "\n",
    "- **`model_name`**: Specifies the backbone model to be fine-tuned. Each model represents a different architecture or pretraining configuration for review comment generation.\n",
    "\n",
    "- **`rag_strategy`**: Determines the prompting strategy used during fine-tuning:\n",
    "  - `rag_pair`: Uses both the code and its corresponding review from retrieved exemplars.\n",
    "  - `rag_singleton`: Uses only the review comments from retrieved exemplars.\n",
    "  - `vanilla`: No retrieval augmentation; uses only the input code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80afb5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model_name among \"Tufano\", \"CodeReviewer\", \"CodeT5\", \"CodeT5p-220m\", \"Auger\"\n",
    "model_name = \"CodeT5\" # here to modify\n",
    "# Choose strategy among \"rag_pair\", \"rag_singleton\", \"vanilla\"\n",
    "rag_strategy = \"rag_pair\" # here to modify\n",
    "\n",
    "dataset_base = '../../dataset/'\n",
    "retrieval_base = \"../retrieval/rag_candidate/\"\n",
    "output_ckp_base = './output/fine_tuned_checkpoints/'\n",
    "path_train = dataset_base+'train.tsv'\n",
    "path_val = dataset_base+'val.tsv'\n",
    "total_topk = 30\n",
    "\n",
    "if rag_strategy == \"rag_pair\":\n",
    "    top_k =  8 \n",
    "else:\n",
    "    top_k = 30\n",
    "\n",
    "batch_size = 12\n",
    "max_input_length=512\n",
    "max_target_length=128\n",
    "num_beams = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dd37fc",
   "metadata": {},
   "source": [
    "Upload model and tokenizer according to the model_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e324c362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AutoTokenizer, AutoModelForSeq2SeqLM, RobertaTokenizer\n",
    "\n",
    "if model_name == \"Tufano\":\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"./pre-trained_checkpoints/TufanoT5_pre-trained/pytorch_version\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"./pre-trained_checkpoints/TufanoT5_tokenizer/TokenizerModel.model\")\n",
    "elif model_name == \"CodeReviewer\":\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"microsoft/codereviewer\")\n",
    "    tokenizer =  AutoTokenizer.from_pretrained(\"microsoft/codereviewer\")\n",
    "elif model_name == \"CodeT5\":\n",
    "    model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "elif model_name == \"CodeT5p-220m\":\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"Salesforce/codet5p-220m\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5p-220m\")\n",
    "else:\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"SEBIS/code_trans_t5_base_code_documentation_generation_java_multitask\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"SEBIS/code_trans_t5_base_code_documentation_generation_java_multitask\")\n",
    "    model.resize_token_embeddings(32101)\n",
    "    state_dict = torch.load(\"./pre-trained_checkpoints/AUGER_pre-trained/best_ppl_pretraining_pytorch.bin\")\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Model loaded on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86422026",
   "metadata": {},
   "source": [
    "Load train and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40539c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDataset(dataset):\n",
    "    code_list = []\n",
    "    comment_list = []\n",
    "    for data in dataset:\n",
    "        split_index = data.rfind(\"\\t\")\n",
    "        code = data[:split_index] \n",
    "        comment = data[split_index + 1:]  \n",
    "        code_list.append(code)\n",
    "        comment_list.append(comment)\n",
    "    return code_list, comment_list\n",
    "\n",
    "train_dataset = [line.strip() for line in open(path_train)]\n",
    "val_dataset = [line.strip() for line in open(path_val)]\n",
    "\n",
    "train_code, train_comment = processDataset(train_dataset)\n",
    "val_code, val_comment = processDataset(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251c0fc9",
   "metadata": {},
   "source": [
    "Load retrieval candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a096b014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_candidates(topk, total_topk, file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        file_lines = f.read().splitlines()\n",
    "\n",
    "    num_instances = len(file_lines) // total_topk\n",
    "    retrieved_file = [\n",
    "    file_lines[i * total_topk:(i + 1) * total_topk][:topk] for i in range(num_instances)\n",
    "    ]\n",
    "\n",
    "    return retrieved_file\n",
    "\n",
    "train_top30_candidate_comment_file = os.path.join(retrieval_base, f\"train_to_train_retrieval_top30_comment.txt\")\n",
    "train_top30_candidate_code_file = os.path.join(retrieval_base, f\"train_to_train_retrieval_top30_code.txt\")\n",
    "val_top30_candidate_comment_file = os.path.join(retrieval_base, f\"val_to_train_retrieval_top30_comment.txt\")\n",
    "val_top30_candidate_code_file = os.path.join(retrieval_base, f\"val_to_train_retrieval_top30_code.txt\")\n",
    "\n",
    "train_candidiate_comment = get_topk_candidates(top_k, total_topk, train_top30_candidate_comment_file)\n",
    "train_candidiate_code = get_topk_candidates(top_k, total_topk, train_top30_candidate_code_file)\n",
    "val_candidiate_comment = get_topk_candidates(top_k, total_topk, val_top30_candidate_comment_file)\n",
    "val_candidiate_code = get_topk_candidates(top_k, total_topk, val_top30_candidate_code_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9328b773",
   "metadata": {},
   "source": [
    "Build RAG inputs. \n",
    "\n",
    "If rag_strategy is \"vanilla\", it does not augment any retrieval candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24612c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_inputs(rag_strategy, inputs, candidate_comment, candidate_code):\n",
    "    rag_inputs = []\n",
    "    for i in tqdm(range(len(inputs)), desc=\"Building RAG inputs\"):\n",
    "        x = inputs[i]\n",
    "        topk_codes = candidate_code[i]\n",
    "        topk_comments = candidate_comment[i]\n",
    "        if rag_strategy == \"rag_singleton\":\n",
    "            for comment in topk_comments:\n",
    "                x += \"[nsep]\" + comment\n",
    "        elif rag_strategy == \"rag_pair\":\n",
    "            for j in range(len(topk_codes)):\n",
    "                x += \"[nsep]\" + topk_comments[j] + \"[csep]\" + topk_codes[j]\n",
    "        rag_inputs.append(x)\n",
    "    return rag_inputs\n",
    "\n",
    "train_rag_input = build_rag_inputs(rag_strategy, train_code, train_candidiate_comment, train_candidiate_code, max_input_length)\n",
    "val_rag_input = build_rag_inputs(rag_strategy, val_code, val_candidiate_comment, val_candidiate_code, max_input_length)\n",
    "train_target = train_comment\n",
    "val_target = val_comment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babf6665",
   "metadata": {},
   "source": [
    "Construct dataloader for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be740521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuneDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, tokenizer, max_input_length=512, max_target_length=128):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.inputs[idx]\n",
    "        target_text = self.targets[idx]\n",
    "        source_enc = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_input_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        target_enc = self.tokenizer(\n",
    "            target_text,\n",
    "            max_length=self.max_target_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        source_enc = {k: v.squeeze(0) for k, v in source_enc.items()}\n",
    "        target_ids = target_enc[\"input_ids\"].squeeze(0)\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100\n",
    "        return source_enc, target_ids\n",
    "    \n",
    "train_dataset = FineTuneDataset(train_rag_input, train_target, tokenizer, max_input_length, max_target_length)\n",
    "val_dataset = FineTuneDataset(val_rag_input, val_target, tokenizer, max_input_length, max_target_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805c3bc",
   "metadata": {},
   "source": [
    "Define validation function.\n",
    "\n",
    "Validation is based on the Exact Match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4c200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import bleu_score\n",
    "import statistics\n",
    "\n",
    "chencherry = bleu_score.SmoothingFunction()\n",
    "\n",
    "def validate_model():\n",
    "    model.eval()\n",
    "    perfect_predictions = 0\n",
    "    BLEUscore = []\n",
    "    total_samples = len(val_dataset)\n",
    "\n",
    "    outputs, targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, desc=\"Validating\"):\n",
    "            inputs, target = batch\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            target = target.to(device) \n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_target_length,\n",
    "                num_beams=num_beams, \n",
    "                num_return_sequences=1,  \n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "            pred_texts = [tokenizer.decode(gid, skip_special_tokens=True) for gid in generated_ids]\n",
    "\n",
    "            target_texts = [\n",
    "                tokenizer.decode(t[t >= 0].tolist(), skip_special_tokens=True) for t in target\n",
    "            ]\n",
    "\n",
    "            outputs.extend(pred_texts)\n",
    "            targets.extend(target_texts)\n",
    "    \n",
    "            for pred, target in zip(pred_texts, target_texts):\n",
    "                if \" \".join(pred.split()) == \" \".join(target.split()):\n",
    "                    perfect_predictions += 1\n",
    "                BLEUscore.append(bleu_score.sentence_bleu([target], pred, smoothing_function=chencherry.method1))\n",
    "                \n",
    "    pp_percentage = (perfect_predictions * 100) / total_samples\n",
    "    print(f'Perfect Prediction (PP): {perfect_predictions}/{total_samples} ({pp_percentage:.2f}%)')\n",
    "    print('BLEU mean:', statistics.mean(BLEUscore))\n",
    "\n",
    "    return perfect_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ecdcdf",
   "metadata": {},
   "source": [
    "Fine-tune the pre-trained language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afae9164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_ckp():\n",
    "    best_ckp_path = os.path.join(output_ckp_base, model_name + \"_\" + rag_strategy + f\"_finetuned_best_ckp_{best_ckp_epoch}\")\n",
    "    model.save_pretrained(best_ckp_path)\n",
    "    tokenizer.save_pretrained(best_ckp_path)\n",
    "    print(f\"Saved best_ckp model for epoch {best_ckp_epoch}\")\n",
    "\n",
    "def save_ckp(epoch):\n",
    "    ckp_path = os.path.join(output_ckp_base, model_name + \"_\" + rag_strategy + f\"_finetuned_ckp_{epoch}\")\n",
    "    model.save_pretrained(ckp_path)\n",
    "    tokenizer.save_pretrained(ckp_path)\n",
    "    print(f\"Saved ckp model for epoch {epoch}\")\n",
    "\n",
    "def remove_best_ckp():\n",
    "    if best_pp == 0:\n",
    "        return\n",
    "    best_ckp_path = os.path.join(output_ckp_base, model_name + \"_\" + rag_strategy + f\"_finetuned_best_ckp_{best_ckp_epoch}\")\n",
    "    shutil.rmtree(best_ckp_path)\n",
    "    print(f\"Removed best_ckp model for epoch {best_ckp_epoch}\")\n",
    "\n",
    "\n",
    "start_epoch = 1\n",
    "end_epoch = 20\n",
    "patience = 3\n",
    "best_pp = 0\n",
    "best_ckp_epoch = 0\n",
    "epochs_since_improvement = 0 \n",
    "\n",
    "\n",
    "if model_name == \"Tufano\" or model_name == \"Auger\":\n",
    "    learning_rate = 0.0003\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(start_epoch, end_epoch + 1):\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch}/{end_epoch}\"):\n",
    "            inputs, labels = batch\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        avg_loss = epoch_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch}/{end_epoch}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        save_ckp(epoch)\n",
    "        curr_pp = validate_model()\n",
    "\n",
    "        if curr_pp > best_pp:\n",
    "            remove_best_ckp()\n",
    "            best_pp = curr_pp\n",
    "            best_ckp_epoch = epoch\n",
    "            save_best_ckp()\n",
    "            epochs_since_improvement = 0 \n",
    "            print(f\"New best PP: {best_pp} at epoch {best_ckp_epoch}\")\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "            print(f\"No improvement for {epochs_since_improvement} epoch(s).\")\n",
    "\n",
    "        if epochs_since_improvement >= patience:\n",
    "            print(f\"Early stopping triggered after {patience} epochs without improvement.\")\n",
    "            break\n",
    "else:\n",
    "    learning_rate = 3e-5\n",
    "    accumulation_steps = 3 \n",
    "    weight_decay = 0.01\n",
    "    warmup_ratio = 0.1\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    total_steps = len(train_dataloader) * (end_epoch - start_epoch + 1) // accumulation_steps\n",
    "    warmup_steps = int(total_steps * warmup_ratio)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "    scaler = GradScaler() \n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(start_epoch, end_epoch + 1):\n",
    "        epoch_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch}/{end_epoch}\")):\n",
    "            inputs, labels = batch\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with autocast(): \n",
    "                outputs = model(**inputs, labels=labels)\n",
    "                loss = outputs.loss / accumulation_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            epoch_loss += loss.item() * accumulation_steps\n",
    "\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch}/{end_epoch}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        save_ckp(epoch)\n",
    "        curr_pp = validate_model()\n",
    "\n",
    "        if curr_pp > best_pp:\n",
    "            remove_best_ckp()\n",
    "            best_pp = curr_pp\n",
    "            best_ckp_epoch = epoch\n",
    "            save_best_ckp()\n",
    "            epochs_since_improvement = 0\n",
    "            print(f\"New best PP: {best_pp} at epoch {best_ckp_epoch}\")\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "            print(f\"No improvement for {epochs_since_improvement} epoch(s).\")\n",
    "\n",
    "        if epochs_since_improvement >= patience:\n",
    "            print(f\"Early stopping triggered after {patience} epochs without improvement.\")\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
