{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08438f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.translate import bleu_score\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "from transformers.utils import logging\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f68fd55",
   "metadata": {},
   "source": [
    "This section defines the required setup for evaluation. Users must manually set the `model_name`, `rag_strategy`, `path_fine_tuned_ckp` and `path_inference_output` variables in the code (modify them in the below section).\n",
    "\n",
    "- **`model_name`**: Specifies the fine-tuned backbone model to use. Each model represents a different architecture or pretraining configuration for review comment generation.\n",
    "\n",
    "- **`rag_strategy`**: Determines the prompting strategy used during inference (same as fine-tuned model):\n",
    "  - `rag_pair`: Uses both the code and its corresponding review from retrieved exemplars.\n",
    "  - `rag_singleton`: Uses only the review comments from retrieved exemplars.\n",
    "  - `vanilla`: No retrieval augmentation; uses only the input code.\n",
    "\n",
    "- **`path_fine_tuned_ckp`**: Path to the fine-tuned model checkpoint.\n",
    "\n",
    "- **`path_inference_output`**: Path to the file containing inference results (to be loaded for evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb23d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model_name among \"Tufano\", \"CodeReviewer\", \"CodeT5\", \"CodeT5p-220m\", \"Auger\"\n",
    "model_name = \"CodeT5\" # here to modify\n",
    "\n",
    "# Choose strategy among \"rag_pair\", \"rag_singleton\", \"vanilla\"\n",
    "rag_strategy = \"rag_pair\" # here to modify\n",
    "\n",
    "# specify the path of fune-tuned model checkpoint to be evaluated. ex) './output/fine_tuned_checkpoints/CodeT5_rag_pair_finetuned_best_ckp_2'\n",
    "path_fine_tuned_ckp = './output/CodeT5_rag_pair_finetuned_best_ckp_2' # here to modify\n",
    "\n",
    "# specify the path where inference output to be loaded. ex) \"./output/inference/\"\n",
    "path_inference_output = \"./output/inference/\" # here to modify\n",
    "\n",
    "dataset_base = '../../dataset/'\n",
    "retrieval_base = \"../retrieval/rag_candidate/\"\n",
    "output_ckp_base = './output/fine_tuned_checkpoints/'\n",
    "path_test = dataset_base+'train.tsv'\n",
    "total_topk = 30\n",
    "\n",
    "if rag_strategy == \"rag_pair\":\n",
    "    top_k =  8 \n",
    "else:\n",
    "    top_k = 30\n",
    "\n",
    "batch_size = 12\n",
    "max_input_length=512\n",
    "max_target_length=128\n",
    "num_beams = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f1f7ea",
   "metadata": {},
   "source": [
    "Upload model and tokenizer according to the path_fine_tuned_ckp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e7d04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AutoTokenizer, AutoModelForSeq2SeqLM, RobertaTokenizer\n",
    "\n",
    "if model_name == \"Tufano\":\n",
    "    model = T5ForConditionalGeneration.from_pretrained(path_fine_tuned_ckp)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(path_fine_tuned_ckp)\n",
    "elif model_name == \"CodeReviewer\":\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(path_fine_tuned_ckp)\n",
    "    tokenizer =  AutoTokenizer.from_pretrained(path_fine_tuned_ckp)\n",
    "elif model_name == \"CodeT5\":\n",
    "    model = T5ForConditionalGeneration.from_pretrained(path_fine_tuned_ckp)\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(path_fine_tuned_ckp)\n",
    "elif model_name == \"CodeT5p-220m\":\n",
    "    model = T5ForConditionalGeneration.from_pretrained(path_fine_tuned_ckp)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path_fine_tuned_ckp)\n",
    "else:\n",
    "    model = T5ForConditionalGeneration.from_pretrained(path_fine_tuned_ckp)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(path_fine_tuned_ckp)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Model loaded on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c67325",
   "metadata": {},
   "source": [
    "Load test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a94f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDataset(dataset):\n",
    "    code_list = []\n",
    "    comment_list = []\n",
    "    for data in dataset:\n",
    "        split_index = data.rfind(\"\\t\")\n",
    "        code = data[:split_index] \n",
    "        comment = data[split_index + 1:]  \n",
    "        code_list.append(code)\n",
    "        comment_list.append(comment)\n",
    "    return code_list, comment_list\n",
    "\n",
    "test_dataset = [line.strip() for line in open(path_test)]\n",
    "\n",
    "test_code, test_comment = processDataset(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8459ec",
   "metadata": {},
   "source": [
    "Load retrieval candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e9291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_candidates(topk, total_topk, file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        file_lines = f.read().splitlines()\n",
    "\n",
    "    num_instances = len(file_lines) // total_topk\n",
    "    retrieved_file = [\n",
    "    file_lines[i * total_topk:(i + 1) * total_topk][:topk] for i in range(num_instances)\n",
    "    ]\n",
    "    \n",
    "    return retrieved_file\n",
    "\n",
    "test_top30_candidate_comment_file = os.path.join(dataset_base, f\"test_to_train_retrieval_top30_comment.txt\")\n",
    "test_top30_candidate_code_file = os.path.join(dataset_base, f\"test_to_train_retrieval_top30_code.txt\")\n",
    "\n",
    "test_candidate_comment = get_topk_candidates(top_k, total_topk, test_top30_candidate_comment_file)\n",
    "test_candidate_code = get_topk_candidates(top_k, total_topk, test_top30_candidate_code_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b849a97d",
   "metadata": {},
   "source": [
    "Build RAG inputs. \n",
    "\n",
    "If rag_strategy is \"vanilla\", it does not augment any retrieval candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfc5bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_inputs(rag_strategy, inputs, candidate_comment, candidate_code):\n",
    "    rag_inputs = []\n",
    "    for i in tqdm(range(len(inputs)), desc=\"Building RAG inputs\"):\n",
    "        x = inputs[i]\n",
    "        topk_codes = candidate_code[i]\n",
    "        topk_comments = candidate_comment[i]\n",
    "        if rag_strategy == \"rag_singleton\":\n",
    "            for comment in topk_comments:\n",
    "                x += \"[nsep]\" + comment\n",
    "        elif rag_strategy == \"rag_pair\":\n",
    "            for j in range(len(topk_codes)):\n",
    "                x += \"[nsep]\" + topk_comments[j] + \"[csep]\" + topk_codes[j]\n",
    "        rag_inputs.append(x)\n",
    "    return rag_inputs\n",
    "\n",
    "test_rag_input = build_rag_inputs(rag_strategy, test_code, test_candidate_comment, test_candidate_code, max_input_length)\n",
    "test_target = test_comment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcf4aac",
   "metadata": {},
   "source": [
    "Construct dataloader for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cc4ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuneDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, tokenizer, max_input_length=512, max_target_length=128):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.inputs[idx]\n",
    "        target_text = self.targets[idx]\n",
    "        source_enc = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_input_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        target_enc = self.tokenizer(\n",
    "            target_text,\n",
    "            max_length=self.max_target_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        source_enc = {k: v.squeeze(0) for k, v in source_enc.items()}\n",
    "        target_ids = target_enc[\"input_ids\"].squeeze(0)\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100\n",
    "        return source_enc, target_ids\n",
    "    \n",
    "test_dataset = FineTuneDataset(test_rag_input, test_target, tokenizer, max_input_length, max_target_length)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a2d614",
   "metadata": {},
   "source": [
    "Evaluate the fine-tuned model.\n",
    "\n",
    "Generated review comment (inference output) will be stored in the path_inference_output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c67229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chencherry = bleu_score.SmoothingFunction()\n",
    "\n",
    "def test_model():\n",
    "    model.eval()\n",
    "    perfect_predictions = 0\n",
    "    BLEUscore = []\n",
    "    total_samples = len(test_dataset)\n",
    "\n",
    "    outputs, targets = [], []\n",
    "    prediction_path = os.path.join(path_inference_output, model_name + '_' + rag_strategy + '_predictions.txt')\n",
    "\n",
    "    with torch.no_grad(), open(prediction_path, 'w', encoding='utf-8') as f_out:\n",
    "        for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "            inputs, target = batch\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            target = target.to(device)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_target_length,\n",
    "                num_beams=num_beams,\n",
    "                num_return_sequences=1,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "            pred_texts = [tokenizer.decode(gid, skip_special_tokens=True) for gid in generated_ids]\n",
    "            target_texts = [\n",
    "                tokenizer.decode(t[t >= 0].tolist(), skip_special_tokens=True) for t in target\n",
    "            ]\n",
    "\n",
    "            outputs.extend(pred_texts)\n",
    "            targets.extend(target_texts)\n",
    "\n",
    "            for pred, tgt in zip(pred_texts, target_texts):\n",
    "                f_out.write(pred + '\\n')\n",
    "\n",
    "                if \" \".join(pred.split()) == \" \".join(tgt.split()):\n",
    "                    perfect_predictions += 1\n",
    "                BLEUscore.append(\n",
    "                    bleu_score.sentence_bleu([tgt], pred, smoothing_function=chencherry.method1)\n",
    "                )\n",
    "\n",
    "    pp_percentage = (perfect_predictions * 100) / total_samples\n",
    "    print(f'Perfect Prediction (PP): {perfect_predictions}/{total_samples} ({pp_percentage:.2f}%)')\n",
    "    print('BLEU mean:', statistics.mean(BLEUscore))\n",
    "    print(f'Predictions written to {prediction_path}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
